{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ch-calibration)=\n",
    "# Calibration\n",
    "\n",
    "In the previous [chapter on jets](ch-jets), the jet quantities were all calibrated. The common CP algorithms were used during extraction to:\n",
    "\n",
    "* Calibrate the jets according to the most recent Jet group recommendations that were baked into the release\n",
    "* Calibrate all the other objects (electrons, photons, taus, and muons)\n",
    "* Perform overlap removal\n",
    "\n",
    "This chapter discusses the details of how calibrations are run, and what can be done to configure it. First, however, a short introduction on the calibration model to help understand the technical code.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "It is best to think of an event being calibrated in ATLAS, rather than a single object or collection of objects. In most cases it does not make sense to have one type of object calibrated while another isn't. This is because there are interdependencies between some objects. For example, the calibrated jets, electrons, photons, muons, and taus are all used to calculate the missing $E_T$ for an event. Further, after calibrating jets, electrons, photons, muons, and taus one must run overlap removal to disambiguate, say, jets and electrons. How that calibration is run is called the _calibration configuration_.\n",
    "\n",
    "As a result, it only makes sense to talk about a coherent, calibrated set of objects in an ATLAS event. Further, due to limitations in how `AnalysisBase` (which extracts the data from xAOD's and runs the common CP algorithms) and `func_adl`'s calibration model described here, it is not possible to run with more than a single calibration configuration in a single query. You'll have to split them up into multiple queries.\n",
    "\n",
    "`func_adl` has calibration configuration class. It tells the system what jet collection to use, what muon working point to use, etc. This is configured by default to work well for `DAOD_PHYS`. In the end, the common CP algorithms are configured using the contents of configuration class. If you do nothing, the default configuration for `DAOD_PHYS` is what you'll get. However, if you need a different calibration configuration it is possible to alter this, as we will see below.\n",
    "\n",
    "```{warning}\n",
    "Overlap Removal is disabled for all the tests in these notebooks. We need to figure out how to remove events with no found primary vertices as OR depends on those existing.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found backend type matching \"xaod\". Matching by type is depreciated. Please switch to using the \"name\" keyword in your servicex.yaml file.\n",
      "Found backend type matching \"xaod\". Matching by type is depreciated. Please switch to using the \"name\" keyword in your servicex.yaml file.\n"
     ]
    }
   ],
   "source": [
    "from config import ds_zee_r24 as ds\n",
    "from config import match_eta_phi\n",
    "from func_adl_servicex_xaodr21 import calib_tools\n",
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Calibration Configuration Object\n",
    "\n",
    "The default calibration object details all the collections and working points that are to be used. The default configuration is easy enough to grab and inspect by accessing the `default_config` on the `calib_tools` object. The `calib_tools` object provides access to the configuration and helper methods to modify it in various ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalibrationEventConfig(jet_collection='AntiKt4EMPFlowJets',\n",
      "                       jet_calib_truth_collection='AntiKt4TruthDressedWZJets',\n",
      "                       run_jet_ghost_muon_association=True,\n",
      "                       electron_collection='Electrons',\n",
      "                       electron_working_point='MediumLHElectron',\n",
      "                       electron_isolation='NonIso',\n",
      "                       photon_collection='Photons',\n",
      "                       photon_working_point='Tight',\n",
      "                       photon_isolation='FixedCutTight',\n",
      "                       muon_collection='Muons',\n",
      "                       muon_working_point='Medium',\n",
      "                       muon_isolation='NonIso',\n",
      "                       tau_collection='TauJets',\n",
      "                       tau_working_point='Tight',\n",
      "                       perform_overlap_removal=True,\n",
      "                       datatype='mc',\n",
      "                       correct_pileup=True,\n",
      "                       calibrate=True,\n",
      "                       uncalibrated_possible=True)\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(calib_tools.default_config())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you request `Jet`'s, for example, you'll be getting back fully calibrated and overlap pruned jets. The jets will have started from the `default_config.jet_collection` entry above (these are the same jets we saw in most of the [Jet chapter](sec-jet-simple)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docker image and tag: gitlab-registry.cern.ch/atlas/athena/analysisbase:24.2.6\n",
      "Docker Output: \n",
      "  Configured GCC from: /opt/lcg/gcc/11.2.0-8a51a/x86_64-centos7/bin/gcc\n",
      "  Configured AnalysisBase from: /usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt\n",
      "  -- The C compiler identification is GNU 11.2.0\n",
      "  -- The CXX compiler identification is GNU 11.2.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /opt/lcg/gcc/11.2.0-8a51a/x86_64-centos7/bin/gcc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /opt/lcg/gcc/11.2.0-8a51a/x86_64-centos7/bin/g++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found AnalysisBase: /usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/cmake/AnalysisBaseConfig.cmake (found version \"24.2.6\") \n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
      "  -- Check if compiler accepts -pthread\n",
      "  -- Check if compiler accepts -pthread - yes\n",
      "  -- Found Threads: TRUE  \n",
      "  -- Found GTest: /usr/AnalysisBaseExternals/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/lib/cmake/GTest/GTestConfig.cmake (found version \"1.11.0\")  \n",
      "  -- Found AnalysisBaseExternals: /usr/AnalysisBaseExternals/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/cmake/AnalysisBaseExternalsConfig.cmake (found suitable exact version \"24.2.6\") \n",
      "  -- Setting ATLAS specific build flags\n",
      "  -- checker_gccplugins library not found\n",
      "  -- Using the LCG modules without setting up a release\n",
      "  -- Configuring ATLAS project with name \"func_adl_ntupler\" and version \"\"\n",
      "  -- Using build type: RelWithDebInfo\n",
      "  -- Using platform name: x86_64-centos7-gcc11-opt\n",
      "  -- Cleaning stale files from build area\n",
      "  -- Unit tests will be built by default\n",
      "  -- Found 1 package(s)\n",
      "  -- Considering package 1 / 1\n",
      "  -- No package filtering rules read\n",
      "  -- Configuring the build of package: analysis\n",
      "  -- Number of packages configured: 1\n",
      "  -- Time for package configuration: 0 second(s)\n",
      "  -- Generated file: /workdir/rel/build/x86_64-centos7-gcc11-opt/packages.txt\n",
      "  -- Including the packages from project AnalysisBase - 24.2.6...\n",
      "  -- Including the packages from project AnalysisBaseExternals - 24.2.6...\n",
      "  -- Generated file: /workdir/rel/build/x86_64-centos7-gcc11-opt/compilers.txt\n",
      "  -- Generated file: /workdir/rel/build/x86_64-centos7-gcc11-opt/ReleaseData\n",
      "  -- Generating external environment configuration\n",
      "  -- Writing runtime environment to file: /workdir/rel/build/x86_64-centos7-gcc11-opt/env_setup.sh\n",
      "  (stderr) CMake Warning at /usr/AnalysisBaseExternals/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/cmake/modules/AtlasInternals.cmake:571 (message):\n",
      "  (stderr)   CPack packaging will only work correctly with\n",
      "  (stderr)   CMAKE_INSTALL_PREFIX=/func_adl_ntupler/1.0/InstallArea/x86_64-centos7-gcc11-opt\n",
      "  (stderr) Call Stack (most recent call first):\n",
      "  (stderr)   CMakeLists.txt:54 (atlas_cpack_setup)\n",
      "  (stderr) \n",
      "  (stderr) \n",
      "  -- Configuring done\n",
      "  -- Generating done\n",
      "  -- Build files have been written to: /workdir/rel/build\n",
      "  [  0%] Built target atlas_tests\n",
      "  Scanning dependencies of target queryDictDictGen\n",
      "  [ 12%] Generating queryDictReflexDict.cxx\n",
      "  [ 12%] Built target queryDictDictGen\n",
      "  [ 25%] Built /workdir/rel/build/x86_64-centos7-gcc11-opt/lib/func_adl_ntupler.rootmap\n",
      "  [ 25%] Built target func_adl_ntuplerRootMapMerge\n",
      "  [ 37%] Generating ../x86_64-centos7-gcc11-opt/bin/ATestRun_eljob.py\n",
      "  [ 37%] Built target analysisScriptsInstall\n",
      "  [ 50%] Building CXX object analysis/CMakeFiles/analysisLib.dir/Root/query.cxx.o\n",
      "  [ 62%] Linking CXX shared library ../x86_64-centos7-gcc11-opt/lib/libanalysisLib.so\n",
      "  Detaching debug info of libanalysisLib.so into libanalysisLib.so.dbg\n",
      "  [ 62%] Built target analysisLib\n",
      "  [ 75%] Generating ../x86_64-centos7-gcc11-opt/include/analysis\n",
      "  [ 75%] Built target analysisHeaderInstall\n",
      "  [ 87%] Building CXX object analysis/CMakeFiles/queryDict.dir/CMakeFiles/queryDictReflexDict.cxx.o\n",
      "  [100%] Linking CXX shared library ../x86_64-centos7-gcc11-opt/lib/libqueryDict.so\n",
      "  Detaching debug info of libqueryDict.so into libqueryDict.so.dbg\n",
      "  [100%] Built target queryDict\n",
      "  [100%] Built target Package_analysis\n",
      "  Using calibration cache: /xaod_calibration_cache\n",
      "  Update calibration sources: /xaod_calibration_cache:/workdir/rel/build/x86_64-centos7-gcc11-opt/data:/workdir/rel/build/x86_64-centos7-gcc11-opt/share:/usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/data:/usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/share:/usr/AnalysisBaseExternals/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/data:/usr/AnalysisBaseExternals/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/share:/sw/DbData/GroupData:/cvmfs/atlas.cern.ch/repo/sw/database/GroupData:/eos/atlas/atlascerngroupdisk/asg-calib:http//cern.ch/atlas-groupdata:http//atlas.web.cern.ch/Atlas/GROUPS/DATABASE/GroupData\n",
      "  xAOD::Init                INFO    Environment initialised for data access\n",
      "  SampleHandler with 1 files\n",
      "  Sample:name=ANALYSIS,tags=()\n",
      "  file:///data/DAOD_PHYS.28628223._000007.pool.root.1\n",
      "  \n",
      "  \n",
      "  Py:AutoConfigFlags    INFO Obtaining metadata of auto-configuration by peeking into 'file:///data/DAOD_PHYS.28628223._000007.pool.root.1'\n",
      "  Py:MetaReader        INFO Current mode used: lite\n",
      "  Py:MetaReader        INFO Current filenames: ['file:///data/DAOD_PHYS.28628223._000007.pool.root.1']\n",
      "  Py:MetaReader        INFO MetaReader is called with the parameter \"unique_tag_info_values\" set to True. This is a workaround to remove all duplicate values from \"/TagInfo\" key\n",
      "  Py:AutoConfigFlags    INFO Looking into the file in 'peeker' mode as the configuration requires more details: mc_campaign \n",
      "  Py:MetaReader        INFO Current mode used: peeker\n",
      "  Py:MetaReader        INFO Current filenames: ['file:///data/DAOD_PHYS.28628223._000007.pool.root.1']\n",
      "  Py:MetaReader        INFO MetaReader is called with the parameter \"unique_tag_info_values\" set to True. This is a workaround to remove all duplicate values from \"/TagInfo\" key\n",
      "  PathResolver             WARNING Locating dev file dev/PileupReweighting/share/DSID601xxx/pileup__dsid601189_FS.root. Do not let this propagate to a release\n",
      "  DavixOpen                 ERROR   can not open file \"http://cern.ch/atlas-groupdata/dev/PileupReweighting/share/DSID601xxx/pileup__dsid601189_FS.root?filetype=raw&cachesz=4000000&readaheadsz=2000000&rmpolicy=1\" with davix: Result HTTP 404 : File not found  after 3 attempts (18)\n",
      "  TFile::Cp                 ERROR   cannot open source file http://cern.ch/atlas-groupdata/dev/PileupReweighting/share/DSID601xxx/pileup__dsid601189_FS.root\n",
      "  DavixOpen                 ERROR   can not open file \"http://atlas.web.cern.ch/Atlas/GROUPS/DATABASE/GroupData/dev/PileupReweighting/share/DSID601xxx/pileup__dsid601189_FS.root?filetype=raw&cachesz=4000000&readaheadsz=2000000&rmpolicy=1\" with davix: Result HTTP 404 : File not found  after 3 attempts (18)\n",
      "  TFile::Cp                 ERROR   cannot open source file http://atlas.web.cern.ch/Atlas/GROUPS/DATABASE/GroupData/dev/PileupReweighting/share/DSID601xxx/pileup__dsid601189_FS.root\n",
      "  PathResolver             WARNING Could not locate dev/PileupReweighting/share/DSID601xxx/pileup__dsid601189_FS.root\n",
      "  (stderr) Traceback (most recent call last):\n",
      "  (stderr)   File \"/workdir/rel/build/../source/analysis/share/ATestRun_eljob.py\", line 58, in <module>\n",
      "  (stderr)     pileupSequence = makePileupAnalysisSequence(\n",
      "  (stderr)   File \"/usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/python/AsgAnalysisAlgorithms/PileupAnalysisSequence.py\", line 59, in makePileupAnalysisSequence\n",
      "  (stderr)     toolLumicalcFiles = getLumicalcFiles(campaign)\n",
      "  (stderr)   File \"/usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/python/PileupReweighting/AutoconfigurePRW.py\", line 32, in getLumicalcFiles\n",
      "  (stderr)     raise ValueError(f'Unsupported campaign {campaign}')\n",
      "  (stderr) ValueError: Unsupported campaign Campaign.Unknown\n",
      "  \n",
      "ATestRun_eljob.py:\n",
      "  #\n",
      "  # Read the submission directory as a command line argument. You can\n",
      "  # extend the list of arguments with your private ones later on.\n",
      "  # Set up (Py)ROOT.\n",
      "  import ROOT  # type: ignore\n",
      "  import optparse\n",
      "  from AnaAlgorithm.DualUseConfig import createAlgorithm  # type: ignore\n",
      "  \n",
      "  parser = optparse.OptionParser()\n",
      "  \n",
      "  ROOT.xAOD.Init().ignore()\n",
      "  parser.add_option('-s', '--submission-dir', dest='submission_dir',\n",
      "                    action='store', type='string', default='submitDir',\n",
      "                    help='Submission directory for EventLoop')\n",
      "  (options, args) = parser.parse_args()\n",
      "  \n",
      "  \n",
      "  # The sample handler is going to load the files form filelist.txt,\n",
      "  # in this context, it is an embarrassingly easy use of that object.\n",
      "  sh = ROOT.SH.SampleHandler()\n",
      "  sh.setMetaString('nc_tree', 'CollectionTree')\n",
      "  ROOT.SH.readFileList(sh, \"ANALYSIS\", \"filelist.txt\")\n",
      "  sh.printContent()\n",
      "  \n",
      "  # Create an EventLoop job.\n",
      "  job = ROOT.EL.Job()\n",
      "  job.sampleHandler(sh)\n",
      "  \n",
      "  \n",
      "  # pulled from:https://gitlab.cern.ch/atlas/athena/-/blob/21.2/PhysicsAnalysis/Algorithms/JetAnalysisAlgorithms/python/JetAnalysisAlgorithmsTest.py\n",
      "  \n",
      "  # Set up the systematics loader/handler service:\n",
      "  \n",
      "  from AnaAlgorithm.DualUseConfig import createService\n",
      "  \n",
      "  from AnaAlgorithm.AlgSequence import AlgSequence\n",
      "  \n",
      "  calibrationAlgSeq = AlgSequence()\n",
      "  \n",
      "  sysService = createService( 'CP::SystematicsSvc', 'SystematicsSvc', sequence = calibrationAlgSeq )\n",
      "  \n",
      "  sysService.systematicsList = ['NOSYS']\n",
      "  \n",
      "  # Add sequence to job\n",
      "  \n",
      "  \n",
      "  \n",
      "  from AsgAnalysisAlgorithms.PileupAnalysisSequence import makePileupAnalysisSequence\n",
      "  \n",
      "  \n",
      "  \n",
      "  # Use the sh object (sample Handler) to get the first tile and extract the filename\n",
      "  \n",
      "  # from it, which can then be used to fetch the MC campaign. `calib.datatype`\n",
      "  \n",
      "  # should contain `data` or `mc`\n",
      "  \n",
      "  pileupSequence = makePileupAnalysisSequence(\n",
      "  \n",
      "      \"mc\", files=sh.at(0).fileName(0)\n",
      "  \n",
      "  )\n",
      "  \n",
      "  pileupSequence.configure(inputName={}, outputName={})\n",
      "  \n",
      "  print(pileupSequence)  # For debugging\n",
      "  \n",
      "  \n",
      "  \n",
      "  calibrationAlgSeq += pileupSequence\n",
      "  \n",
      "  jetContainer = \"AntiKt4EMPFlowJets\"\n",
      "  \n",
      "  from JetAnalysisAlgorithms.JetAnalysisSequence import makeJetAnalysisSequence\n",
      "  \n",
      "  \n",
      "  \n",
      "  # Do not run ghost muon association if you have already\n",
      "  \n",
      "  # corrected objects.\n",
      "  \n",
      "  jetSequence = makeJetAnalysisSequence(\n",
      "  \n",
      "      \"mc\",\n",
      "  \n",
      "      jetContainer,\n",
      "  \n",
      "      runGhostMuonAssociation=True,\n",
      "  \n",
      "  )\n",
      "  \n",
      "  jetSequence.configure(inputName=jetContainer, outputName=jetContainer + \"_Base_%SYS%\")\n",
      "  \n",
      "  jetSequence.JvtEfficiencyAlg.truthJetCollection = \"AntiKt4TruthDressedWZJets\"\n",
      "  \n",
      "  try:\n",
      "  \n",
      "      jetSequence.ForwardJvtEfficiencyAlg.truthJetCollection = (\n",
      "  \n",
      "          \"AntiKt4TruthDressedWZJets\"\n",
      "  \n",
      "      )\n",
      "  \n",
      "  except AttributeError:\n",
      "  \n",
      "      pass\n",
      "  \n",
      "  \n",
      "  \n",
      "  calibrationAlgSeq += jetSequence\n",
      "  \n",
      "  print(jetSequence)  # For debugging\n",
      "  \n",
      "  \n",
      "  \n",
      "  # Include, and then set up the jet analysis algorithm sequence:\n",
      "  \n",
      "  from JetAnalysisAlgorithms.JetJvtAnalysisSequence import makeJetJvtAnalysisSequence\n",
      "  \n",
      "  \n",
      "  \n",
      "  jvtSequence = makeJetJvtAnalysisSequence(\"mc\", jetContainer, enableCutflow=True)\n",
      "  \n",
      "  jvtSequence.configure(\n",
      "  \n",
      "      inputName={\"jets\": jetContainer + \"_Base_%SYS%\"},\n",
      "  \n",
      "      outputName={\"jets\": jetContainer + \"Calib_%SYS%\"},\n",
      "  \n",
      "  )\n",
      "  \n",
      "  calibrationAlgSeq += jvtSequence\n",
      "  \n",
      "  print(jvtSequence)  # For debugging\n",
      "  \n",
      "  output_jet_container = \"AntiKt4EMPFlowJetsCalib_%SYS%\"\n",
      "  \n",
      "  # Output jet_collection = AntiKt4EMPFlowJetsCalib_NOSYS\n",
      "  \n",
      "  calibrationAlgSeq.addSelfToJob( job )\n",
      "  \n",
      "  print(job) # for debugging\n",
      "  \n",
      "  \n",
      "  # Create the algorithm's configuration.\n",
      "  alg = createAlgorithm('query', 'AnalysisAlg')\n",
      "  # later on we'll add some configuration options for our algorithm that go here\n",
      "  \n",
      "  # Add our algorithm to the job\n",
      "  job.algsAdd(alg)\n",
      "  job.outputAdd(ROOT.EL.OutputStream('ANALYSIS'))\n",
      "  \n",
      "  # Run the job using the direct driver.\n",
      "  driver = ROOT.EL.DirectDriver()\n",
      "  driver.submit(job, options.submission_dir)\n",
      "filelist.txt:\n",
      "  /data/DAOD_PHYS.28628223._000007.pool.root.1\n",
      "  \n",
      "package_CMakeLists.txt:\n",
      "  # The name of the package:\n",
      "  project(analysis VERSION 1.0)\n",
      "  atlas_subdir (analysis)\n",
      "  \n",
      "  # Add the shared library:\n",
      "  atlas_add_library (analysisLib\n",
      "    analysis/*.h Root/*.cxx\n",
      "    PUBLIC_HEADERS analysis\n",
      "    LINK_LIBRARIES AnaAlgorithmLib xAODJet )\n",
      "  \n",
      "  if (XAOD_STANDALONE)\n",
      "   # Add the dictionary (for AnalysisBase only):\n",
      "   atlas_add_dictionary (queryDict\n",
      "    analysis/query.h\n",
      "    analysis/selection.xml\n",
      "    LINK_LIBRARIES analysisLib)\n",
      "  endif ()\n",
      "  \n",
      "  if (NOT XAOD_STANDALONE)\n",
      "    # Add a component library for AthAnalysis only:\n",
      "    atlas_add_component (analysis\n",
      "      src/components/*.cxx\n",
      "      LINK_LIBRARIES analysisLib)\n",
      "  endif ()\n",
      "  \n",
      "  # Install files from the package:\n",
      "  atlas_install_scripts( share/*_eljob.py )\n",
      "query.cxx:\n",
      "  #include <analysis/query.h>\n",
      "  #include \"xAODRootAccess/tools/TFileAccessTracer.h\"\n",
      "  \n",
      "  \n",
      "  #include \"xAODJet/JetContainer.h\"\n",
      "  \n",
      "  #include \"xAODJet/versions/Jet_v1.h\"\n",
      "  \n",
      "  \n",
      "  #include <TTree.h>\n",
      "  \n",
      "  query :: query (const std::string& name,\n",
      "                                    ISvcLocator *pSvcLocator)\n",
      "      : EL::AnaAlgorithm (name, pSvcLocator)\n",
      "    \n",
      "  {\n",
      "    // Here you put any code for the base initialization of variables,\n",
      "    // e.g. initialize all pointers to 0.  This is also where you\n",
      "    // declare all properties for your algorithm.  Note that things like\n",
      "    // resetting statistics variables or booking histograms should\n",
      "    // rather go into the initialize() function.\n",
      "  \n",
      "    // Turn off file access statistics reporting. This is, according to Attila, useful\n",
      "    // for GRID jobs, but not so much for other jobs. For those of us not located at CERN\n",
      "    // and for a large amount of data, this can sometimes take a minute.\n",
      "    // So we get rid of it.\n",
      "    xAOD::TFileAccessTracer::enableDataSubmission(false);\n",
      "  \n",
      "    \n",
      "  \n",
      "  }\n",
      "  \n",
      "  StatusCode query :: initialize ()\n",
      "  {\n",
      "    // Here you do everything that needs to be done at the very\n",
      "    // beginning on each worker node, e.g. create histograms and output\n",
      "    // trees.  This method gets called before any input files are\n",
      "    // connected.\n",
      "  \n",
      "    \n",
      "    {\n",
      "    \n",
      "      ANA_CHECK (book (TTree (\"atlas_xaod_tree\", \"My analysis ntuple\")));\n",
      "    \n",
      "      auto myTree = tree (\"atlas_xaod_tree\");\n",
      "    \n",
      "      myTree->Branch(\"pt\", &_pt4);\n",
      "    \n",
      "      myTree->Branch(\"eta\", &_eta5);\n",
      "    \n",
      "      myTree->Branch(\"phi\", &_phi6);\n",
      "    \n",
      "    }\n",
      "    \n",
      "  \n",
      "    \n",
      "  \n",
      "    return StatusCode::SUCCESS;\n",
      "  }\n",
      "  \n",
      "  StatusCode query :: execute ()\n",
      "  {\n",
      "    // Here you do everything that needs to be done on every single\n",
      "    // events, e.g. read input variables, apply cuts, and fill\n",
      "    // histograms and trees.  This is where most of your actual analysis\n",
      "    // code will go.\n",
      "  \n",
      "    \n",
      "    {\n",
      "    \n",
      "      const DataVector<xAOD::Jet_v1>* jets0;\n",
      "    \n",
      "      {\n",
      "    \n",
      "        const DataVector<xAOD::Jet_v1>* result = 0;\n",
      "    \n",
      "        ANA_CHECK (evtStore()->retrieve(result, \"AntiKt4EMPFlowJetsCalib_NOSYS\"));\n",
      "    \n",
      "        jets0 = result;\n",
      "    \n",
      "      }\n",
      "    \n",
      "      for (auto &&i_obj1 : *jets0)\n",
      "    \n",
      "      {\n",
      "    \n",
      "        if (((i_obj1->pt()/1000)>30))\n",
      "    \n",
      "        {\n",
      "    \n",
      "          _pt4.push_back((i_obj1->pt()/1000.0));\n",
      "    \n",
      "        }\n",
      "    \n",
      "      }\n",
      "    \n",
      "      for (auto &&i_obj2 : *jets0)\n",
      "    \n",
      "      {\n",
      "    \n",
      "        if (((i_obj2->pt()/1000)>30))\n",
      "    \n",
      "        {\n",
      "    \n",
      "          _eta5.push_back(i_obj2->eta());\n",
      "    \n",
      "        }\n",
      "    \n",
      "      }\n",
      "    \n",
      "      for (auto &&i_obj3 : *jets0)\n",
      "    \n",
      "      {\n",
      "    \n",
      "        if (((i_obj3->pt()/1000)>30))\n",
      "    \n",
      "        {\n",
      "    \n",
      "          _phi6.push_back(i_obj3->phi());\n",
      "    \n",
      "        }\n",
      "    \n",
      "      }\n",
      "    \n",
      "      tree(\"atlas_xaod_tree\")->Fill();\n",
      "    \n",
      "      _pt4.clear();\n",
      "    \n",
      "      _eta5.clear();\n",
      "    \n",
      "      _phi6.clear();\n",
      "    \n",
      "    }\n",
      "    \n",
      "  \n",
      "    return StatusCode::SUCCESS;\n",
      "  }\n",
      "  \n",
      "  \n",
      "  \n",
      "  StatusCode query :: finalize ()\n",
      "  {\n",
      "    // This method is the mirror image of initialize(), meaning it gets\n",
      "    // called after the last event has been processed on the worker node\n",
      "    // and allows you to finish up any objects you created in\n",
      "    // initialize() before they are written to disk.  This is actually\n",
      "    // fairly rare, since this happens separately for each worker node.\n",
      "    // Most of the time you want to do your post-processing on the\n",
      "    // submission node after all your histogram outputs have been\n",
      "    // merged.\n",
      "    return StatusCode::SUCCESS;\n",
      "  }\n",
      "query.h:\n",
      "  #ifndef analysis_query_H\n",
      "  #define analysis_query_H\n",
      "  \n",
      "  #include <AnaAlgorithm/AnaAlgorithm.h>\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  class query : public EL::AnaAlgorithm\n",
      "  {\n",
      "  public:\n",
      "    // this is a standard algorithm constructor\n",
      "    query (const std::string& name, ISvcLocator* pSvcLocator);\n",
      "  \n",
      "    // these are the functions inherited from Algorithm\n",
      "    virtual StatusCode initialize () override;\n",
      "    virtual StatusCode execute () override;\n",
      "    virtual StatusCode finalize () override;\n",
      "  \n",
      "  private:\n",
      "    // Class level variables\n",
      "  \n",
      "    \n",
      "    std::vector<double> _pt4;\n",
      "  \n",
      "    \n",
      "    std::vector<double> _eta5;\n",
      "  \n",
      "    \n",
      "    std::vector<double> _phi6;\n",
      "  \n",
      "    \n",
      "  \n",
      "    \n",
      "  };\n",
      "  \n",
      "  #endif\n",
      "runner.sh:\n",
      "  #!/bin/env bash\n",
      "  \n",
      "  # If any problem occurs during the running of this script we want to bail and make sure that\n",
      "  # everyone above us knows what happened.\n",
      "  set -e\n",
      "  \n",
      "  # Meant to be invokved in an ATLAS R22 analysis container.\n",
      "  # This follows the tutorial from https://atlassoftwaredocs.web.cern.ch/ABtutorial/release_setup/\n",
      "  \n",
      "  # Parse the command line arguments. Our defaults\n",
      "  output_method=\"cp\"\n",
      "  output_dir=\"/results\"\n",
      "  input_method=\"filelist\"\n",
      "  input_file=\"\"\n",
      "  compile=1\n",
      "  run=1\n",
      "  calib_cache=\"/xaod_calibration_cache\"\n",
      "  \n",
      "  while getopts \"d:o:cr\" opt; do\n",
      "      case \"$opt\" in\n",
      "      d)\n",
      "          input_method=\"cmd\"\n",
      "          input_file=$OPTARG\n",
      "          ;;\n",
      "      c)\n",
      "          run=0\n",
      "          ;;\n",
      "      r)\n",
      "          compile=0\n",
      "          ;;\n",
      "      o)\n",
      "          output_dir=$OPTARG\n",
      "          ;;\n",
      "      ?)\n",
      "          exit 10\n",
      "      esac\n",
      "  done\n",
      "  \n",
      "  # If there are any arguments left over, then very bad things have happened.\n",
      "  shift $((OPTIND-1))\n",
      "  if [ $# != 0 ]; then\n",
      "    echo \"Extra arguments on the command line $@\"\n",
      "    exit 1\n",
      "  fi\n",
      "  \n",
      "  # Setup and config\n",
      "  source /home/atlas/release_setup.sh\n",
      "  \n",
      "  # Remember where we are and the script location.\n",
      "  DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n",
      "  local=`pwd`\n",
      "  \n",
      "  # Create a release directory\n",
      "  if [ $compile = 1 ]; then\n",
      "     mkdir rel\n",
      "     cd rel\n",
      "     mkdir source\n",
      "     mkdir build\n",
      "     mkdir run\n",
      "  \n",
      "  # Create cmake infrastructure\n",
      "     cat > source/CMakeLists.txt << 'EOF'\n",
      "  #\n",
      "  # Project configuration for UserAnalysis.\n",
      "  #\n",
      "  project(func_adl_ntupler)\n",
      "  \n",
      "  # Set the minimum required CMake version:\n",
      "  cmake_minimum_required( VERSION 3.4 FATAL_ERROR )\n",
      "  \n",
      "  # Try to figure out what project is our parent. Just using a hard-coded list\n",
      "  # of possible project names. Basically the names of all the other\n",
      "  # sub-directories inside the Projects/ directory in the repository.\n",
      "  set( _parentProjectNames Athena AthenaP1 AnalysisBase AthAnalysis\n",
      "     AthSimulation AthDerivation AnalysisTop )\n",
      "  set( _defaultParentProject AnalysisBase )\n",
      "  foreach( _pp ${_parentProjectNames} )\n",
      "     if( NOT \"$ENV{${_pp}_DIR}\" STREQUAL \"\" )\n",
      "        set( _defaultParentProject ${_pp} )\n",
      "        break()\n",
      "     endif()\n",
      "  endforeach()\n",
      "  \n",
      "  # Set the parent project name based on the previous findings:\n",
      "  set( ATLAS_PROJECT ${_defaultParentProject}\n",
      "     CACHE STRING \"The name of the parent project to build against\" )\n",
      "  \n",
      "  # Clean up:\n",
      "  unset( _parentProjectNames )\n",
      "  unset( _defaultParentProject )\n",
      "  \n",
      "  # Find the AnalysisBase project. This is what, amongst other things, pulls\n",
      "  # in the definition of all of the \"atlas_\" prefixed functions/macros.\n",
      "  find_package( ${ATLAS_PROJECT} REQUIRED )\n",
      "  \n",
      "  # Set up CTest. This makes sure that per-package build log files can be\n",
      "  # created if the user so chooses.\n",
      "  atlas_ctest_setup()\n",
      "  \n",
      "  # Set up the GitAnalysisTutorial project. With this CMake will look for \"packages\"\n",
      "  # in the current repository and all of its submodules, respecting the\n",
      "  # \"package_filters.txt\" file, and set up the build of those packages.\n",
      "  atlas_project( UserAnalysis 1.0.0\n",
      "     USE ${ATLAS_PROJECT} ${${ATLAS_PROJECT}_VERSION} )\n",
      "  \n",
      "  # Set up the runtime environment setup script. This makes sure that the\n",
      "  # project's \"setup.sh\" script can set up a fully functional runtime environment,\n",
      "  # including all the externals that the project uses.\n",
      "  lcg_generate_env( SH_FILE ${CMAKE_BINARY_DIR}/${ATLAS_PLATFORM}/env_setup.sh )\n",
      "  install( FILES ${CMAKE_BINARY_DIR}/${ATLAS_PLATFORM}/env_setup.sh\n",
      "     DESTINATION . )\n",
      "  \n",
      "  # Set up CPack. This call makes sure that an RPM or TGZ file can be created\n",
      "  # from the built project. Used by Panda to send the project to the grid worker\n",
      "  # nodes.\n",
      "  atlas_cpack_setup()\n",
      "  EOF\n",
      "  \n",
      "     # Create a package infrastructure\n",
      "     cd source\n",
      "     mkdir analysis\n",
      "     mkdir analysis/analysis\n",
      "     mkdir analysis/Root\n",
      "     mkdir analysis/src\n",
      "     mkdir analysis/src/components\n",
      "     mkdir analysis/share\n",
      "  \n",
      "     # Create the basics for cmake\n",
      "     cp $DIR/package_CMakeLists.txt analysis/CMakeLists.txt\n",
      "  \n",
      "     # Next, copy over the algorithm. The source directory needs to be correctly mounted.\n",
      "     cp $DIR/query.h analysis/analysis\n",
      "     cp $DIR/query.cxx analysis/Root\n",
      "     cp $DIR/ATestRun_eljob.py analysis/share\n",
      "     chmod +x analysis/share/ATestRun_eljob.py\n",
      "  \n",
      "     cat > analysis/analysis/queryDict.h << EOF\n",
      "  #ifndef analysis_query_DICT_H\n",
      "  #define analysis_query_DICT_H\n",
      "  \n",
      "  // This file includes all the header files that you need to create\n",
      "  // dictionaries for.\n",
      "  \n",
      "  #include <analysis/query.h>\n",
      "  \n",
      "  #endif\n",
      "  EOF\n",
      "  \n",
      "     cat > analysis/analysis/selection.xml << EOF\n",
      "  <lcgdict>\n",
      "  \n",
      "    <!-- This file contains a list of all classes for which a dictionary\n",
      "         should be created. -->\n",
      "  \n",
      "    <class name=\"query\" />\n",
      "     \n",
      "  </lcgdict>\n",
      "  EOF\n",
      "  \n",
      "  \n",
      "     # Do the build\n",
      "     cd ../build\n",
      "     cmake ../source\n",
      "     make\n",
      "  else\n",
      "     cd rel/build\n",
      "  fi\n",
      "  \n",
      "  # Sort out the input file location\n",
      "  if [ $run = 1 ]; then\n",
      "     source ${AnalysisBaseExternals_PLATFORM}/setup.sh\n",
      "     if [ \"$input_method\" == \"filelist\" ]; then\n",
      "        if [ -e $DIR/filelist.txt ]; then\n",
      "           cp $DIR/filelist.txt .\n",
      "        else\n",
      "           cp $local/filelist.txt .\n",
      "        fi\n",
      "     elif [ \"$input_method\" == \"cmd\" ]; then\n",
      "        echo $input_file > filelist.txt\n",
      "     fi\n",
      "  \n",
      "     # Do the run\n",
      "     if [ -e ./bogus ]; then\n",
      "       rm -rf bogus\n",
      "     fi\n",
      "  \n",
      "     # If there is a calibration path, lets try to use it.\n",
      "     if [ -e $calib_cache ]; then\n",
      "        export CALIBPATH=$calib_cache:$CALIBPATH\n",
      "        sudo -i chmod a+w $calib_cache\n",
      "        echo \"Using calibration cache: $calib_cache\"\n",
      "        echo \"Update calibration sources: $CALIBPATH\"\n",
      "     fi\n",
      "  \n",
      "     # Finally, run!\n",
      "     python ../source/analysis/share/ATestRun_eljob.py --submission-dir=bogus\n",
      "  \n",
      "     # Place the output file where it belongs\n",
      "     if [ $output_method == \"cp\" ]; then\n",
      "        cmd=\"cp\"\n",
      "        destination=$output_dir\n",
      "     else\n",
      "        destination=$1\n",
      "        cmd=\"cp\"\n",
      "        if [[ $destination == \"root:\"* ]]; then\n",
      "           cmd=\"xrdcp\"\n",
      "        fi\n",
      "     fi\n",
      "     $cmd ./bogus/data-ANALYSIS/ANALYSIS.root $destination\n",
      "  fi\n"
     ]
    },
    {
     "ename": "DockerException",
     "evalue": "The docker command executed was `C:\\Program Files\\Docker\\Docker\\resources\\bin\\docker.EXE container run --rm --volume C:\\Users\\gordo\\AppData\\Local\\Temp\\tmp3e_c71gf:/scripts:ro --volume C:\\Users\\gordo\\AppData\\Local\\Temp\\tmp3e_c71gf:/results: --volume C:\\Users\\gordo\\Code\\atlas\\data\\R22\\DAOD_PHYS:/data/:ro --volume func_adl_atlas_xaod_calibration_cache:/xaod_calibration_cache gitlab-registry.cern.ch/atlas/athena/analysisbase:24.2.6 /scripts/runner.sh`.\nIt returned with code 1\nThe content of stdout can be found above the stacktrace (it wasn't captured).\nThe content of stderr is 'CMake Warning at /usr/AnalysisBaseExternals/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/cmake/modules/AtlasInternals.cmake:571 (message):\n  CPack packaging will only work correctly with\n  CMAKE_INSTALL_PREFIX=/func_adl_ntupler/1.0/InstallArea/x86_64-centos7-gcc11-opt\nCall Stack (most recent call first):\n  CMakeLists.txt:54 (atlas_cpack_setup)\n\n\nTraceback (most recent call last):\n  File \"/workdir/rel/build/../source/analysis/share/ATestRun_eljob.py\", line 58, in <module>\n    pileupSequence = makePileupAnalysisSequence(\n  File \"/usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/python/AsgAnalysisAlgorithms/PileupAnalysisSequence.py\", line 59, in makePileupAnalysisSequence\n    toolLumicalcFiles = getLumicalcFiles(campaign)\n  File \"/usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/python/PileupReweighting/AutoconfigurePRW.py\", line 32, in getLumicalcFiles\n    raise ValueError(f'Unsupported campaign {campaign}')\nValueError: Unsupported campaign Campaign.Unknown\n'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDockerException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m pflow_jets \u001b[39m=\u001b[39m (ds\n\u001b[0;32m      2\u001b[0m         \u001b[39m.\u001b[39;49mSelect(\u001b[39mlambda\u001b[39;49;00m e: e\u001b[39m.\u001b[39;49mJets())\n\u001b[0;32m      3\u001b[0m         \u001b[39m.\u001b[39;49mSelect(\u001b[39mlambda\u001b[39;49;00m jets: jets\u001b[39m.\u001b[39;49mWhere(\u001b[39mlambda\u001b[39;49;00m j: (j\u001b[39m.\u001b[39;49mpt() \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m) \u001b[39m>\u001b[39;49m \u001b[39m30\u001b[39;49m))\n\u001b[0;32m      4\u001b[0m         \u001b[39m.\u001b[39;49mSelect(\u001b[39mlambda\u001b[39;49;00m jets: {\n\u001b[0;32m      5\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m: [j\u001b[39m.\u001b[39;49mpt()\u001b[39m/\u001b[39;49m\u001b[39m1000.0\u001b[39;49m \u001b[39mfor\u001b[39;49;00m j \u001b[39min\u001b[39;49;00m jets],\n\u001b[0;32m      6\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39meta\u001b[39;49m\u001b[39m'\u001b[39;49m: [j\u001b[39m.\u001b[39;49meta() \u001b[39mfor\u001b[39;49;00m j \u001b[39min\u001b[39;49;00m jets],\n\u001b[0;32m      7\u001b[0m                 \u001b[39m'\u001b[39;49m\u001b[39mphi\u001b[39;49m\u001b[39m'\u001b[39;49m: [j\u001b[39m.\u001b[39;49mphi() \u001b[39mfor\u001b[39;49;00m j \u001b[39min\u001b[39;49;00m jets],\n\u001b[0;32m      8\u001b[0m         })\n\u001b[0;32m      9\u001b[0m         \u001b[39m.\u001b[39;49mAsAwkwardArray()\n\u001b[1;32m---> 10\u001b[0m         \u001b[39m.\u001b[39;49mvalue())\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\make_it_sync\\func_wrapper.py:63\u001b[0m, in \u001b[0;36mmake_sync.<locals>.wrapped_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m _sync_version_of_function(fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\make_it_sync\\func_wrapper.py:26\u001b[0m, in \u001b[0;36m_sync_version_of_function\u001b[1;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m exector \u001b[39m=\u001b[39m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m future \u001b[39m=\u001b[39m exector\u001b[39m.\u001b[39msubmit(get_data_wrapper, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 26\u001b[0m \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\make_it_sync\\func_wrapper.py:21\u001b[0m, in \u001b[0;36m_sync_version_of_function.<locals>.get_data_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m asyncio\u001b[39m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m     20\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m loop\u001b[39m.\u001b[39mis_running()\n\u001b[1;32m---> 21\u001b[0m \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py:649\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m future\u001b[39m.\u001b[39mdone():\n\u001b[0;32m    647\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 649\u001b[0m \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\func_adl\\object_stream.py:406\u001b[0m, in \u001b[0;36mObjectStream.value_async\u001b[1;34m(self, executor, title)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[39m# Run it\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunc_adl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mast\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmeta_data\u001b[39;00m \u001b[39mimport\u001b[39;00m remove_empty_metadata\n\u001b[1;32m--> 406\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m exe(remove_empty_metadata(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_q_ast), title)\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\func_adl_servicex\\ServiceX.py:252\u001b[0m, in \u001b[0;36mServiceXDatasetSourceBase.execute_result_async\u001b[1;34m(self, a, title)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39m# Run the query for real!\u001b[39;00m\n\u001b[0;32m    251\u001b[0m attr \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ds, name)\n\u001b[1;32m--> 252\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m attr(q_str, title\u001b[39m=\u001b[39mtitle)\n\u001b[0;32m    254\u001b[0m \u001b[39m# If this is a single column awkward query, and the user did not specify a column name, then\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[39m# we will return the first column.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    257\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mawkward\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m name\n\u001b[0;32m    258\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m has_col_names(a))\n\u001b[0;32m    259\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mkey=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcol1\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(result\u001b[39m.\u001b[39mlayout)\n\u001b[0;32m    260\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex_utils.py:51\u001b[0m, in \u001b[0;36m_wrap_in_memory_sx_cache.<locals>.cached_version_of_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mh\u001b[39m}\u001b[39;00m\u001b[39m - processing request\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m     sx\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mset_inmem(h, result)\n\u001b[0;32m     53\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex.py:436\u001b[0m, in \u001b[0;36mServiceXDataset.get_data_awkward_async\u001b[1;34m(self, selection_query, title)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(ServiceXABC\u001b[39m.\u001b[39mget_data_awkward_async, updated\u001b[39m=\u001b[39m())\n\u001b[0;32m    430\u001b[0m \u001b[39m@_wrap_in_memory_sx_cache\u001b[39m\n\u001b[0;32m    431\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mget_data_awkward_async\u001b[39m(\n\u001b[0;32m    432\u001b[0m     \u001b[39mself\u001b[39m, selection_query: \u001b[39mstr\u001b[39m, title: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    433\u001b[0m ):\n\u001b[0;32m    434\u001b[0m     data_format \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_types[\u001b[39m0\u001b[39m]\n\u001b[0;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_converter\u001b[39m.\u001b[39mcombine_awkward(\n\u001b[1;32m--> 436\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_return(\n\u001b[0;32m    437\u001b[0m             selection_query,\n\u001b[0;32m    438\u001b[0m             \u001b[39mlambda\u001b[39;00m f: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_converter\u001b[39m.\u001b[39mconvert_to_awkward(f),\n\u001b[0;32m    439\u001b[0m             title,\n\u001b[0;32m    440\u001b[0m             data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[0;32m    441\u001b[0m         )\n\u001b[0;32m    442\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\backoff\\_async.py:151\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m details \u001b[39m=\u001b[39m {\n\u001b[0;32m    143\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m: target,\n\u001b[0;32m    144\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m: args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39melapsed\u001b[39m\u001b[39m\"\u001b[39m: elapsed,\n\u001b[0;32m    148\u001b[0m }\n\u001b[0;32m    150\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    152\u001b[0m \u001b[39mexcept\u001b[39;00m exception \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    153\u001b[0m     giveup_result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m giveup(e)\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\backoff\\_async.py:151\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m details \u001b[39m=\u001b[39m {\n\u001b[0;32m    143\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m: target,\n\u001b[0;32m    144\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m: args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39melapsed\u001b[39m\u001b[39m\"\u001b[39m: elapsed,\n\u001b[0;32m    148\u001b[0m }\n\u001b[0;32m    150\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    152\u001b[0m \u001b[39mexcept\u001b[39;00m exception \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    153\u001b[0m     giveup_result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m giveup(e)\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex.py:721\u001b[0m, in \u001b[0;36mServiceXDataset._data_return\u001b[1;34m(self, selection_query, converter, title, data_format)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m@on_exception\u001b[39m(backoff\u001b[39m.\u001b[39mconstant, ServiceXUnknownRequestID, interval\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, max_tries\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m    688\u001b[0m \u001b[39m@on_exception\u001b[39m(\n\u001b[0;32m    689\u001b[0m     backoff\u001b[39m.\u001b[39mconstant,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    699\u001b[0m     data_format: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mroot-file\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    700\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[0;32m    701\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a query, return the data, in a unique order, that hold\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[39m    the data for the query.\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39m                            on the converter call.\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     all_data \u001b[39m=\u001b[39m {\n\u001b[0;32m    722\u001b[0m         f\u001b[39m.\u001b[39mfile: f\u001b[39m.\u001b[39mdata\n\u001b[0;32m    723\u001b[0m         \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream_return(\n\u001b[0;32m    724\u001b[0m             selection_query, title, converter, data_format\n\u001b[0;32m    725\u001b[0m         )\n\u001b[0;32m    726\u001b[0m     }\n\u001b[0;32m    728\u001b[0m     \u001b[39m# Finally, we need them in the proper order so we append them\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[39m# all together\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     ordered_data \u001b[39m=\u001b[39m [all_data[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(all_data\u001b[39m.\u001b[39mkeys())]\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex.py:721\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m@on_exception\u001b[39m(backoff\u001b[39m.\u001b[39mconstant, ServiceXUnknownRequestID, interval\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, max_tries\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m    688\u001b[0m \u001b[39m@on_exception\u001b[39m(\n\u001b[0;32m    689\u001b[0m     backoff\u001b[39m.\u001b[39mconstant,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    699\u001b[0m     data_format: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mroot-file\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    700\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[0;32m    701\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a query, return the data, in a unique order, that hold\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[39m    the data for the query.\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39m                            on the converter call.\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     all_data \u001b[39m=\u001b[39m {\n\u001b[0;32m    722\u001b[0m         f\u001b[39m.\u001b[39mfile: f\u001b[39m.\u001b[39mdata\n\u001b[0;32m    723\u001b[0m         \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream_return(\n\u001b[0;32m    724\u001b[0m             selection_query, title, converter, data_format\n\u001b[0;32m    725\u001b[0m         )\n\u001b[0;32m    726\u001b[0m     }\n\u001b[0;32m    728\u001b[0m     \u001b[39m# Finally, we need them in the proper order so we append them\u001b[39;00m\n\u001b[0;32m    729\u001b[0m     \u001b[39m# all together\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     ordered_data \u001b[39m=\u001b[39m [all_data[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(all_data\u001b[39m.\u001b[39mkeys())]\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex.py:764\u001b[0m, in \u001b[0;36mServiceXDataset._stream_return\u001b[1;34m(self, selection_query, title, converter, data_format)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given a query, return the data, in the order it arrives back\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[39mconverted as appropriate.\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[39m                        on the converter call.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    759\u001b[0m as_data \u001b[39m=\u001b[39m (\n\u001b[0;32m    760\u001b[0m     StreamInfoData(f\u001b[39m.\u001b[39mfile, \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mensure_future(converter(f\u001b[39m.\u001b[39mpath)))\n\u001b[0;32m    761\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream_local_files(selection_query, title, data_format)\n\u001b[0;32m    762\u001b[0m )  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m as_data:\n\u001b[0;32m    765\u001b[0m     \u001b[39myield\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex.py:759\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_stream_return\u001b[39m(\n\u001b[0;32m    735\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    736\u001b[0m     selection_query: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m     data_format: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mroot-file\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    740\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AsyncIterator[StreamInfoData]:\n\u001b[0;32m    741\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given a query, return the data, in the order it arrives back\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[39m    converted as appropriate.\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[39m                            on the converter call.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m     as_data \u001b[39m=\u001b[39m (\n\u001b[0;32m    760\u001b[0m         StreamInfoData(f\u001b[39m.\u001b[39mfile, \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mensure_future(converter(f\u001b[39m.\u001b[39mpath)))\n\u001b[0;32m    761\u001b[0m         \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream_local_files(selection_query, title, data_format)\n\u001b[0;32m    762\u001b[0m     )  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m as_data:\n\u001b[0;32m    765\u001b[0m         \u001b[39myield\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex.py:799\u001b[0m, in \u001b[0;36mServiceXDataset._stream_local_files\u001b[1;34m(self, selection_query, title, data_format)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[39m# Get all the files\u001b[39;00m\n\u001b[0;32m    792\u001b[0m as_files \u001b[39m=\u001b[39m (\n\u001b[0;32m    793\u001b[0m     f\n\u001b[0;32m    794\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_files(\n\u001b[0;32m    795\u001b[0m         selection_query, data_format, notifier, title\n\u001b[0;32m    796\u001b[0m     )\n\u001b[0;32m    797\u001b[0m )  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m name, a_path \u001b[39min\u001b[39;00m as_files:\n\u001b[0;32m    800\u001b[0m     \u001b[39myield\u001b[39;00m StreamInfoPath(name, Path(\u001b[39mawait\u001b[39;00m a_path))\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex.py:792\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    789\u001b[0m notifier \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_notifier(title, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    791\u001b[0m \u001b[39m# Get all the files\u001b[39;00m\n\u001b[1;32m--> 792\u001b[0m as_files \u001b[39m=\u001b[39m (\n\u001b[0;32m    793\u001b[0m     f\n\u001b[0;32m    794\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_files(\n\u001b[0;32m    795\u001b[0m         selection_query, data_format, notifier, title\n\u001b[0;32m    796\u001b[0m     )\n\u001b[0;32m    797\u001b[0m )  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m name, a_path \u001b[39min\u001b[39;00m as_files:\n\u001b[0;32m    800\u001b[0m     \u001b[39myield\u001b[39;00m StreamInfoPath(name, Path(\u001b[39mawait\u001b[39;00m a_path))\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex.py:838\u001b[0m, in \u001b[0;36mServiceXDataset._get_files\u001b[1;34m(self, selection_query, data_format, notifier, title)\u001b[0m\n\u001b[0;32m    834\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_json_query(selection_query, data_format, title)\n\u001b[0;32m    836\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m aiohttp\u001b[39m.\u001b[39mClientSession() \u001b[39mas\u001b[39;00m client:\n\u001b[0;32m    837\u001b[0m     \u001b[39m# Get a request id - which might be cached, but if not, submit it.\u001b[39;00m\n\u001b[1;32m--> 838\u001b[0m     request_id \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_request_id(client, query)\n\u001b[0;32m    840\u001b[0m     \u001b[39m# Make sure cache status exists (user could have deleted, see #176)\u001b[39;00m\n\u001b[0;32m    841\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mquery_status_exists(request_id):\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\servicex\\servicex.py:911\u001b[0m, in \u001b[0;36mServiceXDataset._get_request_id\u001b[1;34m(self, client, query)\u001b[0m\n\u001b[0;32m    909\u001b[0m request_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mlookup_query(query)\n\u001b[0;32m    910\u001b[0m \u001b[39mif\u001b[39;00m request_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 911\u001b[0m     request_info \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_servicex_adaptor\u001b[39m.\u001b[39msubmit_query(client, query)\n\u001b[0;32m    912\u001b[0m     request_id \u001b[39m=\u001b[39m request_info[\u001b[39m\"\u001b[39m\u001b[39mrequest_id\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    913\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mset_query(query, request_id)\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\func_adl_servicex\\local_dataset.py:92\u001b[0m, in \u001b[0;36m_sx_local_file_adaptor.submit_query\u001b[1;34m(self, _, json_query)\u001b[0m\n\u001b[0;32m     90\u001b[0m query \u001b[39m=\u001b[39m json_query[\u001b[39m'\u001b[39m\u001b[39mselection\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     91\u001b[0m title \u001b[39m=\u001b[39m json_query[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m json_query \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 92\u001b[0m file_list \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ds\u001b[39m.\u001b[39mexecute_result_async(\n\u001b[0;32m     93\u001b[0m     text_ast_to_python_ast(query)\u001b[39m.\u001b[39mbody[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mvalue,\n\u001b[0;32m     94\u001b[0m     title\n\u001b[0;32m     95\u001b[0m )\n\u001b[0;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_minio\u001b[39m.\u001b[39massociate_file(request_id, file_list)\n\u001b[0;32m     99\u001b[0m \u001b[39m# Return the info needed from the submit!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\func_adl_xAOD\\common\\local_dataset.py:210\u001b[0m, in \u001b[0;36mLocalDataset.execute_result_async\u001b[1;34m(self, a, title)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m python_on_whales\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mDockerException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dump_info(\n\u001b[0;32m    204\u001b[0m         logging\u001b[39m.\u001b[39mERROR,\n\u001b[0;32m    205\u001b[0m         output,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_docker_image,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    212\u001b[0m \u001b[39m# Now that we have run, we can pluck out the result.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(f_spec\u001b[39m.\u001b[39mresult_rep, cpp_ttree_rep), \u001b[39m\"\u001b[39m\u001b[39mUnknown return type\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\func_adl_xAOD\\common\\local_dataset.py:189\u001b[0m, in \u001b[0;36mLocalDataset.execute_result_async\u001b[1;34m(self, a, title)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     output_generator \u001b[39m=\u001b[39m docker\u001b[39m.\u001b[39mrun(\n\u001b[0;32m    183\u001b[0m         docker_image,\n\u001b[0;32m    184\u001b[0m         [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/scripts/\u001b[39m\u001b[39m{\u001b[39;00mf_spec\u001b[39m.\u001b[39mmain_script\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m         stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    188\u001b[0m     )\n\u001b[1;32m--> 189\u001b[0m     \u001b[39mfor\u001b[39;00m stream_type, stream_content \u001b[39min\u001b[39;00m output_generator:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    190\u001b[0m         \u001b[39mif\u001b[39;00m stream_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstdout\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    191\u001b[0m             output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mstream_content\u001b[39m.\u001b[39mdecode()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\gordo\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\xaod-usage-UR8orL-7-py3.10\\lib\\site-packages\\python_on_whales\\utils.py:264\u001b[0m, in \u001b[0;36mstream_stdout_and_stderr\u001b[1;34m(full_cmd, env)\u001b[0m\n\u001b[0;32m    262\u001b[0m exit_code \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mwait()\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m exit_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m DockerException(full_cmd, exit_code, stderr\u001b[39m=\u001b[39mfull_stderr)\n",
      "\u001b[1;31mDockerException\u001b[0m: The docker command executed was `C:\\Program Files\\Docker\\Docker\\resources\\bin\\docker.EXE container run --rm --volume C:\\Users\\gordo\\AppData\\Local\\Temp\\tmp3e_c71gf:/scripts:ro --volume C:\\Users\\gordo\\AppData\\Local\\Temp\\tmp3e_c71gf:/results: --volume C:\\Users\\gordo\\Code\\atlas\\data\\R22\\DAOD_PHYS:/data/:ro --volume func_adl_atlas_xaod_calibration_cache:/xaod_calibration_cache gitlab-registry.cern.ch/atlas/athena/analysisbase:24.2.6 /scripts/runner.sh`.\nIt returned with code 1\nThe content of stdout can be found above the stacktrace (it wasn't captured).\nThe content of stderr is 'CMake Warning at /usr/AnalysisBaseExternals/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/cmake/modules/AtlasInternals.cmake:571 (message):\n  CPack packaging will only work correctly with\n  CMAKE_INSTALL_PREFIX=/func_adl_ntupler/1.0/InstallArea/x86_64-centos7-gcc11-opt\nCall Stack (most recent call first):\n  CMakeLists.txt:54 (atlas_cpack_setup)\n\n\nTraceback (most recent call last):\n  File \"/workdir/rel/build/../source/analysis/share/ATestRun_eljob.py\", line 58, in <module>\n    pileupSequence = makePileupAnalysisSequence(\n  File \"/usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/python/AsgAnalysisAlgorithms/PileupAnalysisSequence.py\", line 59, in makePileupAnalysisSequence\n    toolLumicalcFiles = getLumicalcFiles(campaign)\n  File \"/usr/AnalysisBase/24.2.6/InstallArea/x86_64-centos7-gcc11-opt/python/PileupReweighting/AutoconfigurePRW.py\", line 32, in getLumicalcFiles\n    raise ValueError(f'Unsupported campaign {campaign}')\nValueError: Unsupported campaign Campaign.Unknown\n'\n"
     ]
    }
   ],
   "source": [
    "pflow_jets = (ds\n",
    "        .Select(lambda e: e.Jets())\n",
    "        .Select(lambda jets: jets.Where(lambda j: (j.pt() / 1000) > 30))\n",
    "        .Select(lambda jets: {\n",
    "                'pt': [j.pt()/1000.0 for j in jets],\n",
    "                'eta': [j.eta() for j in jets],\n",
    "                'phi': [j.phi() for j in jets],\n",
    "        })\n",
    "        .AsAwkwardArray()\n",
    "        .value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ak.flatten(pflow_jets.pt), label='PFlow', bins=100, range=(30, 100))\n",
    "plt.legend()\n",
    "_ = plt.xlabel('Jet $p_T$ [GeV]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the default calibration configuration if you desire. If your analysis is based on a skim, rather than `DAOD_PHYS`, you might want to do this in some master configuration file that everyone includes.\n",
    "\n",
    "All queries that occur after the change will use this new default. Note that calibration configuration is captured at run time (when you use `value` or its equivalent), not when you use the `Jets` method, etc. If you find yourself wanting to use multiple configurations in the same script or notebook, modifying the default is almost certainly not the way to do (see below for other more flexible options).\n",
    "\n",
    "This example modifies the default calibration configuration to use a new jet collection. The identical query above will return a different set of (calibrated) jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_jet_selection = calib_tools.default_config()\n",
    "new_jet_selection.jet_collection = 'AntiKt4EMTopoJets'\n",
    "calib_tools.set_default_config(new_jet_selection)\n",
    "\n",
    "antikt_jets_attempt = (ds\n",
    "                       .Select(lambda e: e.Jets())\n",
    "                       .Select(lambda jets: jets.Where(lambda j: (j.pt() / 1000) > 30))\n",
    "                       .Select(lambda jets: {\n",
    "                               'pt': [j.pt()/1000.0 for j in jets],\n",
    "                               'eta': [j.eta() for j in jets],\n",
    "                               'phi': [j.phi() for j in jets],\n",
    "                       })\n",
    "                       .AsAwkwardArray()\n",
    "                       .value())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that the PFlow algorithm returns more jets than the anti-kt4 algorithm. And that they are mostly at lower jet $p_T$'s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ak.flatten(pflow_jets.pt)), len(ak.flatten(antikt_jets_attempt.pt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait - they are the same. What is going on here?\n",
    "\n",
    "The actual reason is one why it is important not to use this feature except under very limited circumstances. If we look at the code that creates this dataset in `config.py`, we'll see:\n",
    "\n",
    "```python\n",
    "ds = calib_tools.query_update(ds, perform_overlap_removal=False)\n",
    "```\n",
    "\n",
    "That line grabs the default _at the time of the call to `query_update`_ and saves all its values. That occurred during the initial import at the top of the chapter! So us setting this here did nothing! In order to do this, we need to re-run the initial query creation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is how we make a dataset, from the config.py file.\n",
    "from config import make_ds, _samples\n",
    "redone_ds = make_ds(_samples[\"zee_r21\"])\n",
    "\n",
    "antikt_jets = (redone_ds\n",
    "               .Select(lambda e: e.Jets())\n",
    "               .Select(lambda jets: jets.Where(lambda j: (j.pt() / 1000) > 30))\n",
    "               .Select(lambda jets: {\n",
    "                       'pt': [j.pt()/1000.0 for j in jets],\n",
    "                       'eta': [j.eta() for j in jets],\n",
    "                       'phi': [j.phi() for j in jets],\n",
    "               })\n",
    "               .AsAwkwardArray()\n",
    "               .value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ak.flatten(pflow_jets.pt)), len(ak.flatten(antikt_jets.pt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see the expected result - two are different. Again - this is a good reason to use other methods than setting the default. All the pitfalls of global variables will hound you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ak.flatten(pflow_jets.pt), label='PFlow', bins=100, range=(30, 100))\n",
    "plt.hist(ak.flatten(antikt_jets.pt), label='AntiKt4', bins=100, range=(30, 100))\n",
    "plt.legend()\n",
    "_ = plt.xlabel('Jet $p_T$ [GeV]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further see that if we find closest-matching jets in $\\eta-\\phi$, that the algorithms really do gather different amounts of energy (as expected!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pflow_matched_jets_to_antikt = match_eta_phi(antikt_jets, pflow_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ak.flatten(pflow_matched_jets_to_antikt.pt - antikt_jets.pt), bins=100, range=(-10, 10))\n",
    "plt.title('Difference between calibrated PFlow and AntiKt4 jets')\n",
    "_ = plt.xlabel('$\\\\Delta$ Jet $p_T$ [GeV]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can use `calib_tools.reset_config()` to reset to factory default. Though this is mostly for test harneses and the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_tools.reset_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "The `default_config` is a global variable, so it is fairly dangerous to modify unless you really want to change it for everything.\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "Modifying the `default_collection` is particularly dangerous in a notebook where one can execute cells out of order. If you execute the cell above to get `pflow_jets` after the `antikt_jets` cell, you'll get AntiKt4EMToploJets in your pflow jets!\n",
    "```\n",
    "\n",
    "```{admonition} Best practice\n",
    "Use this technique if you are setting the calibration once for your analysis in central a configuration file. Otherwise, ignore it.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the calibrated collection in the query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can modify an aspect of the calibration configuration on a query-by-query basis without using the global `default_config`. There are two ways to do this.\n",
    "\n",
    "First, you can specify the collection to load in place of the default in the `Jets` (or similar) method. This will alter the collection name and run the full calibration. This is useful for quick tests: it is convinent, and keeps the new bank name close to where you make the request - so it is readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jetname_in_query_jets = (ds\n",
    "        .Select(lambda e: e.Jets(\"AntiKt4EMTopoJets\"))\n",
    "        .Select(lambda jets: jets.Where(lambda j: (j.pt() / 1000) > 30))\n",
    "        .Select(lambda jets: {\n",
    "                'pt': [j.pt()/1000.0 for j in jets],\n",
    "                'eta': [j.eta() for j in jets],\n",
    "                'phi': [j.phi() for j in jets],\n",
    "        })\n",
    "        .AsAwkwardArray()\n",
    "        .value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ak.flatten(jetname_in_query_jets.pt)), len(ak.flatten(antikt_jets.pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.sum(abs(jetname_in_query_jets.pt - antikt_jets.pt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this doesn't work well if you want to alter more than calibration configuration value.\n",
    "\n",
    "There is a helper function, `calib_tools.query_update` that allows you to modify values in the calibration configuration. You can also chain the calls, and the modifications will accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_jets_in_query_jets = (calib_tools.query_update(ds, jet_collection='AntiKt4EMTopoJets')\n",
    "        .Select(lambda e: e.Jets())\n",
    "        .Select(lambda jets: jets.Where(lambda j: (j.pt() / 1000) > 30))\n",
    "        .Select(lambda jets: {\n",
    "                'pt': [j.pt()/1000.0 for j in jets],\n",
    "                'eta': [j.eta() for j in jets],\n",
    "                'phi': [j.phi() for j in jets],\n",
    "        })\n",
    "        .AsAwkwardArray()\n",
    "        .value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.sum(abs(config_jets_in_query_jets.pt - antikt_jets.pt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Though the code only changes the `jet_collection` you can change as many values in the calibration configuration object as you like in one call. Just add the names as further arguments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even set a whole new configuration if you have lots of changes at once. This will override anything you've previously set with `query_update` in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_in_query_jets = (calib_tools.query_update(ds, new_jet_selection, perform_overlap_removal=False)\n",
    "        .Select(lambda e: e.Jets())\n",
    "        .Select(lambda jets: jets.Where(lambda j: (j.pt() / 1000) > 30))\n",
    "        .Select(lambda jets: {\n",
    "                'pt': [j.pt()/1000.0 for j in jets],\n",
    "                'eta': [j.eta() for j in jets],\n",
    "                'phi': [j.phi() for j in jets],\n",
    "        })\n",
    "        .AsAwkwardArray()\n",
    "        .value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.sum(abs(config_in_query_jets.pt - antikt_jets.pt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Best practice\n",
    "\n",
    "* Use `query_update` for most cases. It has maximum flexibility and composability. In your analysis, where you declare the dataset, you can easily add a call to `calib_tools.query_update`, for example. You can also modify a query on the fly if you need to alter the calibration configuration for some reason.\n",
    "* Use the `calibrated_collection` argument in a collection request if want to run a one-off test of just a collection change.\n",
    "* Use the `calib_tools.set_default_config` only if you are going to be using this new configuration for everything that will ever be done, and only if you are sure the configuration file being loaded will always by loaded by everyone in the analysis team.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic Errors\n",
    "\n",
    "You can only query a single systematic error at a time. You do this by specifying the systematic error you want using the helper method `calib_tools.query_sys_error`. By default, the central value is returned (`NOSYS`). You will need to know the names of the systematic errors a head of time in order to use this.\n",
    "\n",
    "Here we get jets for evaluated for the systematic error `JET_Pileup_PtTerm_1up`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_jets = (calib_tools.query_sys_error(ds, 'JET_Pileup_PtTerm__1up')\n",
    "            .Select(lambda e: e.Jets())\n",
    "            .Select(lambda jets: jets.Where(lambda j: (j.pt() / 1000) > 30))\n",
    "            .Select(lambda jets: {\n",
    "                    'pt': [j.pt()/1000.0 for j in jets],\n",
    "                    'eta': [j.eta() for j in jets],\n",
    "                    'phi': [j.phi() for j in jets],\n",
    "            })\n",
    "            .AsAwkwardArray()\n",
    "            .value())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare the values to see how big a correction this is by doing the $\\eta-\\phi$ matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_jet_matched = match_eta_phi(pflow_jets, sys_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ak.flatten(pflow_jets.pt - sys_jet_matched.pt), bins=100, range=(-3, 3))\n",
    "plt.title('Effect of the Systematic Error JET_Pileup_PtTerm__1up')\n",
    "_ = plt.xlabel('$\\\\Delta$ Jet $p_T$ [GeV]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this works, it clearly can't be the final way to do this. Espeically given the push model that ATLAS uses for systematic errors. Thoughts welcome at this [issue on how to implement systematic errors](https://github.com/gordonwatts/xaod_usage/issues/13) in the ATLAS xAOD system for `func_adl`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncalibrated Collections\n",
    "\n",
    "You can also request an uncalibrated jet bank - so you can look at the \"raw\" data. For example, we can compare the overlap and calibrated jets above with the jets in the actual starting bank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_jets = calib_tools.default_config().jet_collection\n",
    "uncalibrated_jets = (ds\n",
    "        .Select(lambda e: e.Jets(calibrate=False))\n",
    "        .Select(lambda jets: jets.Where(lambda j: (j.pt() / 1000) > 30))\n",
    "        .Select(lambda jets: {\n",
    "                'pt': [j.pt()/1000.0 for j in jets],\n",
    "                'eta': [j.eta() for j in jets],\n",
    "                'phi': [j.phi() for j in jets],\n",
    "        })\n",
    "        .AsAwkwardArray()\n",
    "        .value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ak.flatten(uncalibrated_jets.pt), label='Raw PFlow', bins=100, range=(30, 100))\n",
    "plt.hist(ak.flatten(pflow_jets.pt), label='PFlow', bins=100, range=(30, 100))\n",
    "plt.legend()\n",
    "_ = plt.xlabel('Jet $p_T$ [GeV]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncalibrated_matched = match_eta_phi(pflow_jets, uncalibrated_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ak.flatten(pflow_jets.pt - uncalibrated_matched.pt), bins=100, range=(-10, 10))\n",
    "plt.title('Effect of Calibration on the Jet Collection')\n",
    "_ = plt.xlabel('$\\\\Delta$ Jet $p_T$ [GeV]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AnalysisBase Configuration Code\n",
    "\n",
    "Everyone familiar with AnalysisBase will recognize that the `func_adl_xAOD` system must at some level configure, via python, the common CP tools. The design of the calibration system here potentially allows you to modify that python code as you wish. However, an API has [not yet been developed](https://github.com/gordonwatts/xaod_usage/issues/14) to do this easily.\n",
    "\n",
    "If you are curious about exactly what code is being downloaded, feel free to look at the template files located in the `templates` directory in the `func_adl_servicex_xaodr21` package that is installed in your environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func_adl_servicex_xaodr21 import CalibrationEventConfig\n",
    "help(CalibrationEventConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(calib_tools)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a6a03da167f696356f4586f5aa11d01c71f7eb41b718dba3877a6890ca8074b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('xaod-usage-w68Kx7k0-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
